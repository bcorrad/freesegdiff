<!DOCTYPE html>
<html lang="it">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FreeSeg-Diff</title>
    <style>
        body {
            font-family: 'Times New Roman', Times, serif;
            margin-left: 10%;
            margin-right: 10%;
            padding: 0;
        }

        #image {
            display: block;
            margin: 0 auto;
            max-width: 90%;
        }

        #caption {
            margin-top: 30px;
            text-align: justify;
        }

        #links {
            text-align: center;
        }

        #links a {
            text-decoration: none;
            color: blue;
            text-align: center;
            font-size: small;
        }

        #authors {
            text-align: center;
        }

        #title {
            text-align: center;
        }

        #links h4 {
            display: inline-block;
        }
    </style>
</head>

<body>
    <h1 id="title">FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion
        Models</h1>
    <div id="authors">
        <p>Barbara Toniella Corradini<sup>1</sup> &nbsp; Mustafa Shukor<sup>2</sup> &nbsp; Paul
            Couairon<sup>2</sup> &nbsp; Guillaume Couairon<sup>2</sup> &nbsp;
            <br /> Franco Scarselli<sup>1</sup> &nbsp; Matthieu
            Cord<sup>2,3</sup>
        </p>
        <p><sup>1</sup> University of Siena, <sup>2</sup> Sorbonne University, <sup>3</sup> Valeo.ai</p>
    </div>
    <div id="content">
        <div id="links">
            <h4>
                <a href="link1.html">[Paper]</a>
                <a href="link1.html">[Code]</a>
            </h4>
        </div>
        <img src="imgs/FreeSeg-Diff.png" alt="Immagine" id="image">
        <div id="caption">
            <p>
                <b>Zero-shot open-vocabulary segmentation with FreeSeg-Diff.</b> The image is passed to a diffusion
                model and to an image captioner model to get visual features and text
                description respectively. The features are used to get class-agnostic masks, which are then associated
                with the extracted text. The final segmentation map is obtained after a mask refinement step. All models
                are kept frozen.
            </p>
        </div>
        <img src="imgs/algo.png" alt="Immagine" id="image">
        <div id="caption">
            <p>
                <b>Our detailed pipeline for zero-shot semantic segmentation. </b>
                We extract features from several blocks of the DM corresponding to a noisy image. 
                These features are clustered to generate class-agnostic masks that are used to produce image crops. 
                These cropped parts are then passed to a CLIP image encoder <span style="font-family: 'Times New Roman', Times, serif;">ùì•</span> to associate each mask/cropped image to a given text/class. 
                The textual classes are obtained by generating a caption from BLIP, followed by NLP tools to extract textual entities. 
                These entities are mapped to the most similar mask. 
                Finally, the masks are refined with CRF to obtain the final segmentation map. All models are kept frozen.
            </p>
        </div>
    </div>
</body>

</html>